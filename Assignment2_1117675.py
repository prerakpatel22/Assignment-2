# -*- coding: utf-8 -*-
"""Copy of NLPAssi2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kcH6RIa6PiiNE9QBpNEghf3lryZFaAhq
"""

import pandas as pd
from sklearn.model_selection import train_test_split

data = pd.read_csv("https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv", sep = '\t')


# Import the numpy library to work with and manipulate the data
import numpy as np

# Check the head of the dataframe
data.head(20)

# check the shape of df
data.shape

# Get number of unique sentences
numSentences = data['SentenceId'].max()
# extract full sentences only from the dataset
fullSentences = []
curSentence = 0
for i in range(data.shape[0]):
  if data['SentenceId'][i]> curSentence:
    fullSentences.append((data['Phrase'][i], data['Sentiment'][i]))
    curSentence = curSentence +1

len(fullSentences)

# put data into a df
fullSentDf = pd.DataFrame(fullSentences,
                                columns=['Phrase', 'Sentiment'])

# Check class imbalance in tokenized sentences
data['Sentiment'].value_counts()

# Check class imbalance in full sentences
fullSentDf['Sentiment'].value_counts()

import nltk
import random
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize

documents = []
# Use only complete sentences
for i in range(fullSentDf.shape[0]):
  tmpWords = word_tokenize(fullSentDf['Phrase'][i])
  documents.append((tmpWords, fullSentDf['Sentiment'][i]))

random.seed(9001)
random.shuffle(documents)
print(documents[1][0])

len(documents)

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer
porter = PorterStemmer()
lancaster=LancasterStemmer()
wordnet_lemmatizer = WordNetLemmatizer()
stopwords_en = stopwords.words("english")
punctuations="?:!.,;'\"-()"

#parameters to adjust to see the impact on outcome
remove_stopwords = True
useStemming = False
useLemma = True
removePuncs = True

for l in range(len(documents)):
  label = documents[l][1]
  tmpReview = []
  for w in documents[l][0]:
    newWord = w
    if remove_stopwords and (w in stopwords_en):
      continue
    if removePuncs and (w in punctuations):
      continue
    if useStemming:
      #newWord = porter.stem(newWord)
      newWord = lancaster.stem(newWord)
    if useLemma:
      newWord = wordnet_lemmatizer.lemmatize(newWord)
    tmpReview.append(newWord)
  documents[l] = (' '.join(tmpReview), label)
print(documents[2])

all_data = pd.DataFrame(documents,
                                columns=['text', 'sentiment'])
# Splits the dataset so 70% is used for training and 30% for testing
x_train_raw, x_test_raw, y_train_raw, y_test_raw = train_test_split(all_data['text'], all_data['sentiment'], test_size=0.3, random_state=2003)

len(x_train_raw)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = TfidfVectorizer(stop_words="english", ngram_range=(1, 1))

x_train = vectorizer.fit_transform(x_train_raw)
y_train = y_train_raw
x_test = vectorizer.transform(x_test_raw)
y_test = y_test_raw

# Converts the datasets to numpy arrays to work with our PyTorch model
x_train_np = x_train.toarray()
y_train_np = np.array(y_train)

# Convert the testing data
x_test_np = x_test.toarray()
y_test_np = np.array(y_test)

x_train_np.shape
x_train_np = np.expand_dims(x_train_np, -1)

x_test_np = np.expand_dims(x_test_np, -1)
x_train_np.shape

from keras import backend as K

def recall_m(y_true,y_pred):
  true_positives = K.sum(K.round(K.clip(y_true*y_pred,0,1)))
  possible_positives = K.sum(K.round(K.clip(y_true,0,1)))
  recall = true_positives / (possible_positives + K.epsilon())
  return recall

def precision_m(y_true,y_pred):
  true_positives = K.sum(K.round(K.clip(y_true*y_pred,0,1)))
  predicted_positives = K.sum(K.round(K.clip(y_pred,0,1)))
  precision = true_positives / (predicted_positives + K.epsilon())
  return precision

def f1_m(y_true, y_pred):
  precision = precision_m(y_true, y_pred)
  recall = recall_m(y_true, y_pred)
  return 2*((precision*recall)/(precision+recall+K.epsilon()))

from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Activation
from keras import optimizers
from keras import layers
from keras.utils import to_categorical

def cnn_model(fea_matrix, compiler):
  model = Sequential()
  model.add(Conv1D(filters = 16, kernel_size=2, activation='relu', input_shape = (fea_matrix.shape[1],fea_matrix.shape[2])))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Conv1D(filters = 32, kernel_size=2, activation='relu'))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Conv1D(filters = 64, kernel_size=2, activation='relu'))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Flatten())
  model.add(Activation('relu'))
  model.add(Dense(5))
  model.add(Activation('softmax'))
  model.compile(optimizer = compiler, loss = 'categorical_crossentropy', metrics = ['acc', f1_m, precision_m, recall_m])
  return model

model = cnn_model(x_train_np, optimizers.Nadam(lr = 1e-3))

y_train_np = to_categorical(y_train_np)
y_test_np = to_categorical(y_test_np)
model.fit(x_train_np, y_train_np, batch_size=64, epochs = 30, verbose = 1, validation_split = 0.3)
model.save("1117675_1dconv_reg")

loss, accuracy, f1_score, precision, recall = model.evaluate(x_test_np, y_test_np, verbose = False)

#get_metrics(accuracy,f1_score, precision, recall)
print('Accuracy:',np.round(accuracy,4))
print('Precision:',np.round(precision,4))
print('Recall:',np.round(recall,4))
print('F1 Score:',np.round(f1_score,4))

